{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the req libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import io\n",
    "from ipywidgets import VBox, Layout\n",
    "from tensorflow.keras.preprocessing.image import apply_affine_transform\n",
    "import time\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <h1><center>VGG Face Descriptor Model based Gender Classifier</center></h1>\n",
    "\n",
    "<center><img src=\"head_mast.png\"></center>\n",
    "\n",
    "This web app outputs the probability of a subject in a face centered image being Female. The model was only trained to identify between the genders - female and male. The user can also choose the number of Test Time Augmentation (T.T.A.) to get more accurate predictions.\n",
    "\n",
    "### Directions For Use:\n",
    "1. Select the desired # of Test Time Augmentations using the slider.\n",
    "2. Upload a face centered image of a person.\n",
    "\n",
    "### Outputs\n",
    "1. The user, in realtime, will see the test time augmentations being applied and the corresponding model prediction.\n",
    "2. At the end, The final prediction (avergae of all TTA predictions) is displayed along with the original image uploaded.\n",
    "\n",
    "This is a proof of concept for research purpose ONLY. The model can be used in identification or tagging systems for returning faster query search results by reducing the search space on gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download model weights\n",
    "! python download_gdrive.py 18q2BaRaDNJ7ChLSHm58VV0c0C9h46FT8 vggFaceGender.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip vggFaceGender.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "model = tf.keras.models.load_model(\"vggFaceGender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### widgets\n",
    "##upload button\n",
    "btn_upload = widgets.FileUpload(accept ='image/*'\n",
    "                                , multiple = True\n",
    "                                , align_items='center'\n",
    "                                ,layout=Layout(margin='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## slider for tta\n",
    "sldr = widgets.IntSlider(value=5\n",
    "                         , min=1\n",
    "                         , max=10\n",
    "                         , step=1\n",
    "#                         , description='T.T.A.:'\n",
    "                         , disabled=False\n",
    "                         , continuous_update=False\n",
    "                         , orientation='horizontal'\n",
    "                         , align_items='center'\n",
    "                         , readout=True\n",
    "                         , readout_format='d'\n",
    "                         , layout=Layout(margin='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label predictor\n",
    "out = widgets.Output(layout=Layout(margin='auto'))\n",
    "lbl_pred = widgets.Label(layout=Layout(margin='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the onclick event for upload button\n",
    "def on_click(change):\n",
    "    img = widgets.Image(value = btn_upload.data[-1])\n",
    "    out.clear_output()\n",
    "    with out: display(img)\n",
    "    image1 = Image.open(io.BytesIO(btn_upload.data[-1]))\n",
    "    image = np.array(image1)\n",
    "    aug_img = []\n",
    "    pred_ls =[]\n",
    "    for i in range(sldr.value):\n",
    "        ## convert to tensor \n",
    "        image = tf.convert_to_tensor(image, tf.float32)\n",
    "        image = tf.image.resize(image, [224,224])\n",
    "        image = np.array(image)\n",
    "        ## rotation \n",
    "        transformation = apply_affine_transform(image\n",
    "                                                , theta = 40 * (np.random.normal(0, 1, 1)[0]))\n",
    "        img1 = tf.image.random_flip_left_right(transformation)\n",
    "        \n",
    "        img1 = tf.image.random_hue(img1, 0.02)\n",
    "        img1 = tf.image.random_saturation(img1, 0.7, 1.3)\n",
    "        #img1 = tf.image.random_contrast(img1, 0.8, 1.2)\n",
    "        img1 = tf.image.random_brightness(img1, 0.1)\n",
    "        #img1 = tf.image.random_crop(img1, [250, 250, 3])\n",
    "\n",
    "        img1 = tf.cast(img1, tf.float32) / 255.0\n",
    "        input_arr = tf.image.resize(img1, [224,224])\n",
    "        input_arr = tf.reshape(input_arr, [224,224, 3])\n",
    "        \n",
    "        out.clear_output()\n",
    "        img2 = Image.fromarray(((np.array(input_arr)*255).astype(np.int8)), 'RGB')\n",
    "        with out: display(img2)\n",
    "        input_arr = tf.expand_dims(input_arr, axis=0)\n",
    "        pred = model.predict(input_arr, batch_size=1)[0][0]\n",
    "        pred_ls.append(1-pred)\n",
    "        lbl_pred.value = f'T.T.A. # {i+1} Prediction :{ str(round(1-pred,2))}'\n",
    "        # Wait for 5 seconds\n",
    "        time.sleep(2)\n",
    "    out.clear_output()\n",
    "    img = image1.resize((224,224))\n",
    "    with out: display(img)\n",
    "\n",
    "    ans = str(round(np.mean(pred_ls),2))    \n",
    "    lbl_pred.value = f'Final Result - Probability of Subject being Female :{ ans}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload.observe(on_click, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addf865c05264d4fb089a0d684adcd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select the # Test Time Augmentations ( T.T.A. ) using the slider', layout=Layout(mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## organize widgets\n",
    "display(VBox([widgets.Label('Select the # Test Time Augmentations ( T.T.A. ) using the slider',layout=Layout(margin='auto'))\n",
    "              , sldr\n",
    "              , widgets.Label('Upload your image',layout=Layout(margin='auto'))\n",
    "              , btn_upload\n",
    "              , out\n",
    "              ,lbl_pred\n",
    "              ]\n",
    "             , layout=Layout(border='solid'\n",
    "                             ,width='50%'\n",
    "                             ,margin='auto'))\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
